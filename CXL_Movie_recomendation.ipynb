{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CloudXLab Movie Recomedation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the liberary to setup the pyspark envrionment in python Jupyter notebook\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/usr/spark2.4.3\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/local/anaconda/bin/python\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/local/anaconda/bin/python\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://cxln4.c.thelab-240901.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>cxl_movie_recommender</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fa83c7694e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create the spark session\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"cxl_movie_recommender\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data file \n",
    "Here read the data file using `spark.read.text` not using the `spark.read.csv` as the file have two delaminators and till Spark3 csv except only one, otherwise it gives error.</p>\n",
    "\n",
    "If using spark 3 then it is good to use the csv as we don't need to write couple step which is actually create the data frame from a single value dataframe to the multicolumn(as per data file). Below are the steps\n",
    "- read file as a text which return the Dataframe\n",
    "- now split the data with the delaminator `::` using map and here we have to convert the dataframe to rdd.\n",
    "- Convert RDD to dataframe with the supplied header(as no header in data file) as well.\n",
    "- Now need to convert the data type as well as currently all are `String` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|               value|\n",
      "+--------------------+\n",
      "|1::1193::5::97830...|\n",
      "|1::661::3::978302109|\n",
      "|1::914::3::978301968|\n",
      "|1::3408::4::97830...|\n",
      "|1::2355::5::97882...|\n",
      "+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Read the file from the HDFS folder.\n",
    "# This method return the dataframe with the value as the column header name\n",
    "data_file = spark.read.text(\"/data/ml-1m/ratings.dat\")\n",
    "data_file.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Canvert single column data frame to multi column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using the map create the column by using the split\n",
    "\n",
    "data_rdd = data_file.rdd.map(lambda data: data[0].split('::'))\n",
    "\n",
    "header_col = ['userId', 'movieId', 'rating', 'timestamp']\n",
    "\n",
    "rating_df = data_rdd.toDF(header_col)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### The above code can written as this way as well\n",
    "```python\n",
    "rating_df = data_file.rdd.map(lambda data: data[0].split('::')).toDF(header_col)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+\n",
      "|userId|movieId|rating|timestamp|\n",
      "+------+-------+------+---------+\n",
      "|     1|   1193|     5|978300760|\n",
      "|     1|    661|     3|978302109|\n",
      "|     1|    914|     3|978301968|\n",
      "|     1|   3408|     4|978300275|\n",
      "|     1|   2355|     5|978824291|\n",
      "+------+-------+------+---------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Display the data for checking, but in actual code it is not required as it execute as we are taking action\n",
    "rating_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: string (nullable = true)\n",
      " |-- movieId: string (nullable = true)\n",
      " |-- rating: string (nullable = true)\n",
      " |-- timestamp: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Checking the schema\n",
    "rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Change the data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- userId: integer (nullable = true)\n",
      " |-- movieId: integer (nullable = true)\n",
      " |-- rating: integer (nullable = true)\n",
      " |-- timestamp: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# here chnage the datatype from string to int for all except timestamp which is converted into long\n",
    "\n",
    "#Import the liberary for datatype\n",
    "from pyspark.sql.types import IntegerType, LongType\n",
    "\n",
    "# Here read each column except timestamp for interger conversion.\n",
    "for col in header_col[:-1] :\n",
    "    rating_df = rating_df.withColumn(col, rating_df[col].cast(IntegerType()))\n",
    "\n",
    "rating_df = rating_df.withColumn('timestamp', rating_df['timestamp'].cast(LongType()))\n",
    "\n",
    "# Again print the schema again to check\n",
    "rating_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build the Model\n",
    "###### Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into the train and test of 70 30 ratio\n",
    "train_df, test_df = rating_df.randomSplit([0.7, 0.3], seed=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Build the model using ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the ALS to build the model. Here using the same parameter used by the training.\n",
    "from pyspark.ml.recommendation import ALS\n",
    "als = ALS(maxIter=5, regParam=0.01, userCol='userId', itemCol='movieId', ratingCol='rating')\n",
    "\n",
    "model = als.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model apply on test and preditct the rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(userId=4169, movieId=148, rating=3, timestamp=976588402, prediction=3.5438382625579834),\n",
       " Row(userId=3184, movieId=148, rating=4, timestamp=968708953, prediction=4.268411636352539),\n",
       " Row(userId=1069, movieId=148, rating=2, timestamp=974945135, prediction=3.6641809940338135),\n",
       " Row(userId=1150, movieId=148, rating=2, timestamp=974875106, prediction=2.8830413818359375),\n",
       " Row(userId=3829, movieId=148, rating=2, timestamp=965940170, prediction=3.246178388595581)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predict the rating in the test dataset.\n",
    "predictions = model.transform(test_df)\n",
    "# print the first five row of the prediction\n",
    "predictions.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def print_rmse(pred_df):\n",
    "    result = pred_df.rdd.map(lambda data: math.pow(data[2]-data[4], 2)).filter(lambda x: not math.isnan(x))\n",
    "    result = result.reduce(lambda x,y: x+y)\n",
    "    rmse = math.sqrt(result/pred_df.count())\n",
    "\n",
    "    print('rmse of the model is {}%'.format(round(rmse, 2)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse of the model is 91.0%\n"
     ]
    }
   ],
   "source": [
    "print_rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation and Hyper parameter tuning\n",
    "\n",
    "By using this cross validation and hyper parameter can find the best model\n",
    "\n",
    "- Here set the different parameter in range so that cross validation use every combination and evalute the result based on the provided evaluator.\n",
    "- After geeting the best model on first iteration tune the paramter again and run the cross validation again. \n",
    "- Need todo this process till that look there is not much chnages.\n",
    "- Here I am doing only two iteration as demostration purpose. Also not putting much params as it take huge time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the require liberary.\n",
    "\n",
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Combine all steps in a method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is a method which run the cross validation and then print best params\n",
    "def model_tunning(estimator, estimatorParams, evaluator, dataset) :\n",
    "    cv = CrossValidator(estimator=estimator, estimatorParamMaps=estimatorParams, evaluator=evaluator, numFolds=5)\n",
    "\n",
    "    model = cv.fit(dataset)\n",
    "    \n",
    "    print('---------Best params---------')\n",
    "    print('Rank:', model.bestModel._java_obj.parent().getRank())\n",
    "    print('RegParam:', model.bestModel._java_obj.parent().getRegParam())\n",
    "    \n",
    "    return model.bestModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Iteration 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Best params---------\n",
      "Rank: 5\n",
      "RegParam: 0.01\n"
     ]
    }
   ],
   "source": [
    "# setting up the initail hyper parameter for tunning\n",
    "hyper_param = ParamGridBuilder()\\\n",
    "                            .addGrid(als.rank, [x for x in range(5, 15, 5) ])\\\n",
    "                            .addGrid(als.regParam, [x * 0.01 for x in range(1, 4, 2)])\\\n",
    "                            .build()\n",
    "\n",
    "# Setting up the evaluation matrics so that cross validation use to find the best one.\n",
    "evaluator = RegressionEvaluator(metricName='rmse', labelCol=\"rating\", predictionCol=\"prediction\")\n",
    "\n",
    "best_model = model_tunning(als, hyper_param, evaluator, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Iteration 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------Best params---------\n",
      "Rank: 3\n",
      "RegParam: 0.005\n"
     ]
    }
   ],
   "source": [
    "# Tune the parameters\n",
    "hyper_param = ParamGridBuilder()\\\n",
    "                            .addGrid(als.rank, [3, 5])\\\n",
    "                            .addGrid(als.regParam, [0.005, 0.01])\\\n",
    "                            .build()\n",
    "\n",
    "# Here just doing for learning and training purpose as I put more range then it will take more time\n",
    "#second iteration\n",
    "best_model = model_tunning(als, hyper_param, evaluator, train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Model apply on test and preditct the rating "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------+------+---------+----------+\n",
      "|userId|movieId|rating|timestamp|prediction|\n",
      "+------+-------+------+---------+----------+\n",
      "|  4169|    148|     3|976588402|  3.018169|\n",
      "|  3184|    148|     4|968708953| 3.5615373|\n",
      "|  1069|    148|     2|974945135| 3.8229532|\n",
      "|  1150|    148|     2|974875106| 3.1401463|\n",
      "|  3829|    148|     2|965940170| 3.0570917|\n",
      "+------+-------+------+---------+----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "predictions = best_model.transform(test_df)\n",
    "\n",
    "predictions.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### RMSE of the new best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rmse of the model is 89.0%\n"
     ]
    }
   ],
   "source": [
    "print_rmse(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Note: Here the original one gives the better result compare to the results coems from the cross validation. Even on the first iteration it shows the same as original.\n",
    "\n",
    "</p>This is just for the demo purpose. I enjoy this :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
